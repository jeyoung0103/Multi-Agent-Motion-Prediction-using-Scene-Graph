#!/usr/bin/env python3
"""
ë¦¬ìŠ¤íŠ¸ íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ ì„ íƒì ìœ¼ë¡œ SGNet í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì†ë ¥ ê³„ì‚° ê¸°ëŠ¥ ì¶”ê°€
"""

import os
import sys
import pickle
import torch
import numpy as np
from tqdm import tqdm
import argparse

# QCNetonETDì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•˜ê¸° ìœ„í•´ ê²½ë¡œ ì¶”ê°€
sys.path.append('/workspace/ETRITrajPredChallenage')
from qcnet_map_preprocess import ProcessMap

TO_TENSOR_KEYS = ['type', 'position', 'heading', 'valid_mask', 'predict_mask', 'velocity', 'wlh']

def calculate_velocity_from_position(position, valid_mask, dt=0.1):
    """
    position ë°ì´í„°ë¡œë¶€í„° ì†ë ¥ì„ ê³„ì‚°
    
    Args:
        position: (num_agents, num_timesteps, 3) ì¢Œí‘œ ë°ì´í„°
        valid_mask: (num_agents, num_timesteps) ìœ íš¨ì„± ë§ˆìŠ¤í¬
        dt: í”„ë ˆì„ ê°„ ì‹œê°„ ê°„ê²© (ì´ˆ)
    
    Returns:
        velocity: (num_agents, num_timesteps, 3) ì†ë ¥ ë°ì´í„° (km/h)
    """
    num_agents, num_timesteps, _ = position.shape
    velocity = np.zeros_like(position)
    
    for agent_idx in range(num_agents):
        for t in range(num_timesteps):
            if not valid_mask[agent_idx, t]:
                # ìœ íš¨í•˜ì§€ ì•Šì€ í”„ë ˆì„ì€ 0ìœ¼ë¡œ ì„¤ì •
                velocity[agent_idx, t] = [0.0, 0.0, 0.0]
                continue
            
            if t == 0:
                # ì²« ë²ˆì§¸ í”„ë ˆì„ì€ ë‘ ë²ˆì§¸ í”„ë ˆì„ì˜ ì†ë ¥ ì‚¬ìš©
                if t + 1 < num_timesteps and valid_mask[agent_idx, t + 1]:
                    # ë‘ ë²ˆì§¸ í”„ë ˆì„ì˜ ì†ë ¥ ê³„ì‚°
                    pos_curr = position[agent_idx, t, :2]  # x, yë§Œ ì‚¬ìš©
                    pos_next = position[agent_idx, t + 1, :2]
                    displacement = pos_next - pos_curr
                    speed_xy = np.linalg.norm(displacement) / dt  # m/s
                    speed_xy_kmh = speed_xy * 3.6  # km/hë¡œ ë³€í™˜
                    
                    # ë°©í–¥ ë²¡í„° ì •ê·œí™”
                    if speed_xy > 0:
                        direction = displacement / np.linalg.norm(displacement)
                        velocity[agent_idx, t, :2] = direction * speed_xy_kmh
                    else:
                        velocity[agent_idx, t, :2] = [0.0, 0.0]
                else:
                    velocity[agent_idx, t] = [0.0, 0.0, 0.0]
            else:
                # ì´ì „ í”„ë ˆì„ê³¼ì˜ ì†ë ¥ ê³„ì‚°
                if valid_mask[agent_idx, t - 1]:
                    pos_prev = position[agent_idx, t - 1, :2]  # x, yë§Œ ì‚¬ìš©
                    pos_curr = position[agent_idx, t, :2]
                    displacement = pos_curr - pos_prev
                    speed_xy = np.linalg.norm(displacement) / dt  # m/s
                    speed_xy_kmh = speed_xy * 3.6  # km/hë¡œ ë³€í™˜
                    
                    # ë°©í–¥ ë²¡í„° ì •ê·œí™”
                    if speed_xy > 0:
                        direction = displacement / np.linalg.norm(displacement)
                        velocity[agent_idx, t, :2] = direction * speed_xy_kmh
                    else:
                        velocity[agent_idx, t, :2] = [0.0, 0.0]
                else:
                    velocity[agent_idx, t] = [0.0, 0.0, 0.0]
    
    return velocity

def convert_files(source_path, save_path, file_list, description):
    """íŒŒì¼ ë¦¬ìŠ¤íŠ¸ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ë³€í™˜"""
    print(f"\n{description}")
    print(f"Source: {source_path}")
    print(f"Target: {save_path}")
    print(f"Files to convert: {len(file_list)}")
    
    # ì €ì¥ ê²½ë¡œ ìƒì„±
    os.makedirs(save_path, exist_ok=True)
    
    # ë§µ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    map_processor = ProcessMap()
    
    converted_count = 0
    failed_count = 0
    
    for file_name in tqdm(file_list, desc="Converting"):
        try:
            # ì›ë³¸ íŒŒì¼ëª…ì—ì„œ _sgnet.pkl ë˜ëŠ” _qcnet.pkl ì œê±°í•˜ì—¬ ì‹¤ì œ íŒŒì¼ëª… ì°¾ê¸°
            original_name = file_name.replace('_sgnet.pkl', '.pkl').replace('_qcnet.pkl', '.pkl')
            source_file = os.path.join(source_path, original_name)
            
            if not os.path.exists(source_file):
                print(f"Warning: {source_file} not found")
                failed_count += 1
                continue
            
            # ì›ë³¸ ë°ì´í„° ì½ê¸°
            with open(source_file, 'rb') as f:
                data = pickle.load(f)
            
            # ì†ë ¥ ê³„ì‚° (positionê³¼ valid_maskë¥¼ ì‚¬ìš©)
            print(f"  Calculating velocity for {file_name}...")
            calculated_velocity = calculate_velocity_from_position(
                data['agent']['position'], 
                data['agent']['valid_mask']
            )
            data['agent']['velocity'] = calculated_velocity
            
            # numpy to tensor ë³€í™˜
            for key, value in data['agent'].items():
                if key in TO_TENSOR_KEYS:
                    data['agent'][key] = torch.from_numpy(value)
            
            # ë§µ ë°ì´í„° ì „ì²˜ë¦¬
            qcnet_type_map = map_processor(data['map'])
            data['map_polygon'] = qcnet_type_map['map_polygon']
            data['map_point'] = qcnet_type_map['map_point']
            data[('map_point', 'to', 'map_polygon')] = qcnet_type_map[('map_point', 'to', 'map_polygon')]
            data[('map_polygon', 'to', 'map_polygon')] = qcnet_type_map[('map_polygon', 'to', 'map_polygon')]
            
            # ì›ë³¸ map ë°ì´í„° ì œê±°
            data.pop('map')
            
            # ì €ì¥
            save_file = os.path.join(save_path, file_name)
            with open(save_file, 'wb') as f:
                pickle.dump(data, f)
            
            converted_count += 1
            
        except Exception as e:
            print(f"Error converting {file_name}: {e}")
            failed_count += 1
    
    print(f"âœ… Conversion completed: {converted_count} success, {failed_count} failed")
    return converted_count

def load_file_list(list_file):
    """ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸°"""
    if not os.path.exists(list_file):
        print(f"Error: {list_file} not found")
        return []
    
    with open(list_file, 'r') as f:
        files = [line.strip() for line in f if line.strip()]
    
    return files

def main():
    # ê²½ë¡œ ì„¤ì • (Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ ê²½ë¡œ)
    datasets_root = "/workspace/datasets"
    train_source = os.path.join(datasets_root, "train")
    test_source = os.path.join(datasets_root, "test_masked")
    
    # ë¦¬ìŠ¤íŠ¸ íŒŒì¼ë“¤
    train_list_file = "/workspace/QCNetonETD/train_list.txt"
    val_list_file = "/workspace/QCNetonETD/val_list.txt"
    test_flops_list_file = "/workspace/QCNetonETD/test_flops_list.txt"
    
    # 1. val_sgnet ìƒì„± (train í´ë”ì—ì„œ val_list.txt íŒŒì¼ë“¤ë§Œ)
    val_files = load_file_list(val_list_file)
    if val_files:
        convert_files(
            train_source,
            os.path.join(datasets_root, "val_sgnet"),
            val_files,
            "ğŸ”„ Creating val_sgnet from train folder using val_list.txt"
        )
    
    # 2. train_sgnet ì¬ìƒì„± (train í´ë”ì—ì„œ train_list.txt íŒŒì¼ë“¤ë§Œ)
    train_files = load_file_list(train_list_file)
    if train_files:
        # ê¸°ì¡´ train_sgnet ë°±ì—…
        existing_train = os.path.join(datasets_root, "train_sgnet")
        if os.path.exists(existing_train):
            backup_path = os.path.join(datasets_root, "train_sgnet_backup")
            if os.path.exists(backup_path):
                import shutil
                shutil.rmtree(backup_path)
            os.rename(existing_train, backup_path)
            print(f"Backed up existing train_sgnet to train_sgnet_backup")
        
        convert_files(
            train_source,
            existing_train,
            train_files,
            "ğŸ”„ Creating train_sgnet from train folder using train_list.txt"
        )
    
    # 3. test_sgnet ìƒì„± (ëª¨ë“  test_masked íŒŒì¼)
    test_files = [f for f in os.listdir(test_source) if f.endswith('.pkl')]
    test_files_sgnet = [f.replace('.pkl', '_sgnet.pkl') for f in test_files]
    if test_files_sgnet:
        convert_files(
            test_source,
            os.path.join(datasets_root, "test_sgnet"),
            test_files_sgnet,
            "ğŸ”„ Creating test_sgnet from test_masked folder"
        )
    
    # 4. test_flops_sgnet ìƒì„± (test_flops_list.txt íŒŒì¼ë“¤ë§Œ)
    test_flops_files = load_file_list(test_flops_list_file)
    if test_flops_files:
        convert_files(
            test_source,
            os.path.join(datasets_root, "test_flops_sgnet"),
            test_flops_files,
            "ğŸ”„ Creating test_flops_sgnet from test_masked folder using test_flops_list.txt"
        )
    
    print("\nğŸ‰ All conversions completed!")
    
    # ìµœì¢… ê²°ê³¼ í™•ì¸
    print("\nğŸ“Š Final dataset structure:")
    for dirname in ["train_sgnet", "val_sgnet", "test_sgnet", "test_flops_sgnet"]:
        path = os.path.join(datasets_root, dirname)
        if os.path.exists(path):
            count = len([f for f in os.listdir(path) if f.endswith('.pkl')])
            print(f"  {dirname}: {count} files")
        else:
            print(f"  {dirname}: âŒ Not found")

if __name__ == "__main__":
    main()
